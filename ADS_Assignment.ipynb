{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset"
   ],
   "metadata": {
    "id": "P9Q5s_3Sb1Id"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dataset access from: https://archive.ics.uci.edu/dataset/468/online+shoppers+purchasing+intention+dataset"
   ],
   "metadata": {
    "id": "NPD5t9c0byEl"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Online Shoppers Purchasing Intention Dataset\n",
    "Of the 12,330 sessions in the dataset, 84.5% (10,422) were negative class samples that did not end with shopping, and the rest (1908) were positive class samples ending with shopping.\n",
    "\n",
    "Dataset Characteristics\n",
    "Multivariate\n",
    "\n",
    "Subject Area\n",
    "Business\n",
    "\n",
    "Associated Tasks\n",
    "Classification, Clustering\n",
    "\n",
    "Feature Type\n",
    "Integer, Real\n",
    "\n",
    "Instances\n",
    "12330\n",
    "\n",
    "Features\n",
    "17"
   ],
   "metadata": {
    "id": "K7Pc2WTdeKwQ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dataset Information\n",
    "Additional Information\n",
    "\n",
    "The dataset consists of feature vectors belonging to 12,330 sessions.\n",
    "The dataset was formed so that each session\n",
    "would belong to a different user in a 1-year period to avoid\n",
    "any tendency to a specific campaign, special day, user\n",
    "profile, or period.\n",
    "\n",
    "Has Missing Values?\n",
    "\n",
    "No"
   ],
   "metadata": {
    "id": "PN8JUbkDeSAJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install ucimlrepo"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G3XCPVpNbTY3",
    "outputId": "89d56ed2-7b76-46d6-f44b-11039a3e8d14"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting ucimlrepo\n",
      "  Downloading ucimlrepo-0.0.7-py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ucimlrepo) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2020.12.5 in /usr/local/lib/python3.12/dist-packages (from ucimlrepo) (2025.10.5)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.17.0)\n",
      "Downloading ucimlrepo-0.0.7-py3-none-any.whl (8.0 kB)\n",
      "Installing collected packages: ucimlrepo\n",
      "Successfully installed ucimlrepo-0.0.7\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4v1E8GjYXHWN",
    "outputId": "2f33bb9e-bbd8-40f6-d1c3-5ee38be00eed"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'uci_id': 468, 'name': 'Online Shoppers Purchasing Intention Dataset', 'repository_url': 'https://archive.ics.uci.edu/dataset/468/online+shoppers+purchasing+intention+dataset', 'data_url': 'https://archive.ics.uci.edu/static/public/468/data.csv', 'abstract': 'Of the 12,330 sessions in the dataset,\\n84.5% (10,422) were negative class samples that did not\\nend with shopping, and the rest (1908) were positive class\\nsamples ending with shopping.', 'area': 'Business', 'tasks': ['Classification', 'Clustering'], 'characteristics': ['Multivariate'], 'num_instances': 12330, 'num_features': 17, 'feature_types': ['Integer', 'Real'], 'demographics': [], 'target_col': ['Revenue'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 2018, 'last_updated': 'Thu Jan 11 2024', 'dataset_doi': '10.24432/C5F88Q', 'creators': ['C. Sakar', 'Yomi Kastro'], 'intro_paper': {'ID': 367, 'type': 'NATIVE', 'title': 'Real-time prediction of online shoppers‚Äô purchasing intention using multilayer perceptron and LSTM recurrent neural networks', 'authors': 'C. O. Sakar, S. Polat, Mete Katircioglu, Yomi Kastro', 'venue': 'Neural computing & applications (Print)', 'year': 2019, 'journal': None, 'DOI': '10.1007/s00521-018-3523-0', 'URL': 'https://www.semanticscholar.org/paper/747e098f85ca2d20afd6313b11242c0c427e6fb3', 'sha': None, 'corpus': None, 'arxiv': None, 'mag': None, 'acl': None, 'pmid': None, 'pmcid': None}, 'additional_info': {'summary': 'The dataset consists of feature vectors belonging to 12,330 sessions. \\r\\nThe dataset was formed so that each session\\r\\nwould belong to a different user in a 1-year period to avoid\\r\\nany tendency to a specific campaign, special day, user\\r\\nprofile, or period. ', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'The dataset consists of 10 numerical and 8 categorical attributes.\\r\\nThe \\'Revenue\\' attribute can be used as the class label.\\r\\n\\r\\n\"Administrative\", \"Administrative Duration\", \"Informational\", \"Informational Duration\", \"Product Related\" and \"Product Related Duration\" represent the number of different types of pages visited by the visitor in that session and total time spent in each of these page categories. The values of these features are derived from the URL information of the pages visited by the user and updated in real time when a user takes an action, e.g. moving from one page to another. The \"Bounce Rate\", \"Exit Rate\" and \"Page Value\" features represent the metrics measured by \"Google Analytics\" for each page in the e-commerce site. The value of \"Bounce Rate\" feature for a web page refers to the percentage of visitors who enter the site from that page and then leave (\"bounce\") without triggering any other requests to the analytics server during that session. The value of \"Exit Rate\" feature for a specific web page is calculated as for all pageviews to the page, the percentage that were the last in the session. The \"Page Value\" feature represents the average value for a web page that a user visited before completing an e-commerce transaction. The \"Special Day\" feature indicates the closeness of the site visiting time to a specific special day (e.g. Mother‚Äôs Day, Valentine\\'s Day) in which the sessions are more likely to be finalized with transaction. The value of this attribute is determined by considering the dynamics of e-commerce such as the duration between the order date and delivery date. For example, for Valentina‚Äôs day, this value takes a nonzero value between February 2 and February 12, zero before and after this date unless it is close to another special day, and its maximum value of 1 on February 8. The dataset also includes operating system, browser, region, traffic type, visitor type as returning or new visitor, a Boolean value indicating whether the date of the visit is weekend, and month of the year.', 'citation': None}}\n",
      "                       name     role         type demographic description  \\\n",
      "0            Administrative  Feature      Integer        None        None   \n",
      "1   Administrative_Duration  Feature      Integer        None        None   \n",
      "2             Informational  Feature      Integer        None        None   \n",
      "3    Informational_Duration  Feature      Integer        None        None   \n",
      "4            ProductRelated  Feature      Integer        None        None   \n",
      "5   ProductRelated_Duration  Feature   Continuous        None        None   \n",
      "6               BounceRates  Feature   Continuous        None        None   \n",
      "7                 ExitRates  Feature   Continuous        None        None   \n",
      "8                PageValues  Feature      Integer        None        None   \n",
      "9                SpecialDay  Feature      Integer        None        None   \n",
      "10                    Month  Feature  Categorical        None        None   \n",
      "11         OperatingSystems  Feature      Integer        None        None   \n",
      "12                  Browser  Feature      Integer        None        None   \n",
      "13                   Region  Feature      Integer        None        None   \n",
      "14              TrafficType  Feature      Integer        None        None   \n",
      "15              VisitorType  Feature  Categorical        None        None   \n",
      "16                  Weekend  Feature       Binary        None        None   \n",
      "17                  Revenue   Target       Binary        None        None   \n",
      "\n",
      "   units missing_values  \n",
      "0   None             no  \n",
      "1   None             no  \n",
      "2   None             no  \n",
      "3   None             no  \n",
      "4   None             no  \n",
      "5   None             no  \n",
      "6   None             no  \n",
      "7   None             no  \n",
      "8   None             no  \n",
      "9   None             no  \n",
      "10  None             no  \n",
      "11  None             no  \n",
      "12  None             no  \n",
      "13  None             no  \n",
      "14  None             no  \n",
      "15  None             no  \n",
      "16  None             no  \n",
      "17  None             no  \n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# fetch dataset\n",
    "online_shoppers_purchasing_intention_dataset = fetch_ucirepo(id=468)\n",
    "\n",
    "# data (as pandas dataframes)\n",
    "X = online_shoppers_purchasing_intention_dataset.data.features\n",
    "y = online_shoppers_purchasing_intention_dataset.data.targets\n",
    "\n",
    "# metadata\n",
    "print(online_shoppers_purchasing_intention_dataset.metadata)\n",
    "\n",
    "# variable information\n",
    "print(online_shoppers_purchasing_intention_dataset.variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Feature importance for tree-based models\ntree_based_models = ['Random Forest', 'Gradient Boosting', 'XGBoost', 'Decision Tree']\n\nfig, axes = plt.subplots(2, 2, figsize=(18, 14))\naxes = axes.ravel()\n\nfor idx, model_name in enumerate(tree_based_models):\n    if model_name in trained_models:\n        model = trained_models[model_name]\n        \n        # Get feature importances\n        if hasattr(model, 'feature_importances_'):\n            importances = model.feature_importances_\n            \n            # Create dataframe for better visualization\n            feature_imp_df = pd.DataFrame({\n                'Feature': X_train.columns,\n                'Importance': importances\n            }).sort_values('Importance', ascending=False).head(15)\n            \n            # Plot\n            axes[idx].barh(range(len(feature_imp_df)), feature_imp_df['Importance'], \n                          color='steelblue', alpha=0.7)\n            axes[idx].set_yticks(range(len(feature_imp_df)))\n            axes[idx].set_yticklabels(feature_imp_df['Feature'])\n            axes[idx].set_xlabel('Importance', fontsize=11)\n            axes[idx].set_title(f'Top 15 Feature Importances - {model_name}', \n                               fontsize=12, fontweight='bold')\n            axes[idx].invert_yaxis()\n            axes[idx].grid(axis='x', alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# 6. Feature Importance Analysis",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Detailed evaluation of the best model\nbest_model = trained_models[best_model_name]\ny_pred_best = best_model.predict(X_test_scaled)\ny_pred_proba_best = best_model.predict_proba(X_test_scaled)[:, 1]\n\n# Confusion Matrix\ncm = confusion_matrix(y_test, y_pred_best)\n\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# Plot confusion matrix\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0], \n            xticklabels=['No Purchase', 'Purchase'],\n            yticklabels=['No Purchase', 'Purchase'])\naxes[0].set_title(f'Confusion Matrix - {best_model_name}', fontsize=14, fontweight='bold')\naxes[0].set_ylabel('Actual', fontsize=12)\naxes[0].set_xlabel('Predicted', fontsize=12)\n\n# ROC Curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_best)\nroc_auc = roc_auc_score(y_test, y_pred_proba_best)\n\naxes[1].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\naxes[1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\naxes[1].set_xlim([0.0, 1.0])\naxes[1].set_ylim([0.0, 1.05])\naxes[1].set_xlabel('False Positive Rate', fontsize=12)\naxes[1].set_ylabel('True Positive Rate', fontsize=12)\naxes[1].set_title(f'ROC Curve - {best_model_name}', fontsize=14, fontweight='bold')\naxes[1].legend(loc=\"lower right\")\naxes[1].grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Classification Report\nprint(f\"\\nClassification Report - {best_model_name}:\")\nprint(\"=\" * 60)\nprint(classification_report(y_test, y_pred_best, target_names=['No Purchase', 'Purchase']))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Visualize model comparison\nfig, axes = plt.subplots(2, 2, figsize=(16, 12))\n\nmetrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\ncolors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n\nfor idx, metric in enumerate(metrics):\n    ax = axes[idx // 2, idx % 2]\n    results_sorted = results_df.sort_values(metric, ascending=True)\n    \n    ax.barh(results_sorted['Model'], results_sorted[metric], color=colors[idx], alpha=0.7)\n    ax.set_xlabel(metric, fontsize=12, fontweight='bold')\n    ax.set_title(f'Model Comparison - {metric}', fontsize=14, fontweight='bold')\n    ax.set_xlim(0, 1)\n    ax.grid(axis='x', alpha=0.3)\n    \n    # Add value labels\n    for i, v in enumerate(results_sorted[metric]):\n        ax.text(v + 0.01, i, f'{v:.3f}', va='center', fontsize=9)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Create results dataframe\nresults_df = pd.DataFrame(results)\nresults_df = results_df.sort_values('F1-Score', ascending=False).reset_index(drop=True)\n\nprint(\"Model Performance Comparison:\")\nprint(\"=\" * 100)\nprint(results_df.to_string(index=False))\nprint(\"=\" * 100)\n\n# Identify best model\nbest_model_name = results_df.iloc[0]['Model']\nprint(f\"\\nüèÜ Best Model (based on F1-Score): {best_model_name}\")\nprint(f\"   F1-Score: {results_df.iloc[0]['F1-Score']:.4f}\")\nprint(f\"   Accuracy: {results_df.iloc[0]['Accuracy']:.4f}\")\nprint(f\"   ROC-AUC: {results_df.iloc[0]['ROC-AUC']:.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# 5. Model Evaluation & Comparison",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Train and evaluate all models\nresults = []\ntrained_models = {}\n\nprint(\"Training models...\\n\")\nprint(\"=\" * 80)\n\nfor name, model in models.items():\n    print(f\"\\nTraining {name}...\")\n    \n    # Train the model\n    model.fit(X_train_scaled, y_train)\n    \n    # Make predictions\n    y_pred = model.predict(X_test_scaled)\n    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1] if hasattr(model, 'predict_proba') else None\n    \n    # Calculate metrics\n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred)\n    roc_auc = roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else None\n    \n    # Store results\n    results.append({\n        'Model': name,\n        'Accuracy': accuracy,\n        'Precision': precision,\n        'Recall': recall,\n        'F1-Score': f1,\n        'ROC-AUC': roc_auc\n    })\n    \n    # Store trained model\n    trained_models[name] = model\n    \n    print(f\"‚úì {name} trained successfully!\")\n    print(f\"  Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"All models trained successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Define models to train\nmodels = {\n    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n    'Decision Tree': DecisionTreeClassifier(random_state=42),\n    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n    'XGBoost': XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss'),\n    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n    'Naive Bayes': GaussianNB(),\n    'Support Vector Machine': SVC(kernel='rbf', random_state=42, probability=True)\n}\n\nprint(f\"Total models to train: {len(models)}\")\nprint(\"Models:\", list(models.keys()))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Import classification models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\n\nprint(\"All models imported successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# 4. Model Building & Training",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Feature Scaling - StandardScaler\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Convert back to DataFrame for easier handling\nX_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\nX_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n\nprint(\"Scaled training set shape:\", X_train_scaled.shape)\nprint(\"Scaled testing set shape:\", X_test_scaled.shape)\nprint(\"\\nFirst few rows of scaled training data:\")\nprint(X_train_scaled.head())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_prep, y_prep, test_size=0.2, \n                                                      random_state=42, stratify=y_prep)\n\nprint(\"Training set shape:\", X_train.shape)\nprint(\"Testing set shape:\", X_test.shape)\nprint(\"\\nTraining set target distribution:\")\nprint(y_train.value_counts())\nprint(\"\\nTesting set target distribution:\")\nprint(y_test.value_counts())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Encode categorical variables\n# Month: One-hot encoding\nmonth_encoded = pd.get_dummies(X_prep['Month'], prefix='Month', drop_first=True)\n\n# VisitorType: One-hot encoding\nvisitor_encoded = pd.get_dummies(X_prep['VisitorType'], prefix='VisitorType', drop_first=True)\n\n# Weekend: Convert boolean to integer (0, 1)\nX_prep['Weekend'] = X_prep['Weekend'].astype(int)\n\n# Drop original categorical columns and add encoded ones\nX_prep = X_prep.drop(['Month', 'VisitorType'], axis=1)\nX_prep = pd.concat([X_prep, month_encoded, visitor_encoded], axis=1)\n\n# Convert target variable to integer\ny_prep = y_prep.astype(int)\n\nprint(\"Processed dataset shape:\", X_prep.shape)\nprint(\"\\nNew feature columns:\")\nprint(X_prep.columns.tolist())\nprint(f\"\\nTotal features: {X_prep.shape[1]}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Create a copy of the dataframe for preprocessing\ndf_processed = df.copy()\n\n# Separate features and target\nX_prep = df_processed.drop('Revenue', axis=1)\ny_prep = df_processed['Revenue']\n\nprint(\"Original dataset shape:\", X_prep.shape)\nprint(\"Target variable shape:\", y_prep.shape)\nprint(\"\\nFeature columns:\")\nprint(X_prep.columns.tolist())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# 3. Data Preprocessing",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Analyze categorical features vs Revenue\nfig, axes = plt.subplots(2, 2, figsize=(16, 10))\n\n# Month vs Revenue\npd.crosstab(df['Month'], df['Revenue'], normalize='index').plot(kind='bar', ax=axes[0, 0], \n                                                                  color=['#FF6B6B', '#4ECDC4'])\naxes[0, 0].set_title('Purchase Rate by Month', fontsize=12, fontweight='bold')\naxes[0, 0].set_xlabel('Month', fontsize=10)\naxes[0, 0].set_ylabel('Proportion', fontsize=10)\naxes[0, 0].legend(['No Purchase', 'Purchase'])\naxes[0, 0].tick_params(axis='x', rotation=45)\n\n# VisitorType vs Revenue\npd.crosstab(df['VisitorType'], df['Revenue'], normalize='index').plot(kind='bar', ax=axes[0, 1],\n                                                                        color=['#FF6B6B', '#4ECDC4'])\naxes[0, 1].set_title('Purchase Rate by Visitor Type', fontsize=12, fontweight='bold')\naxes[0, 1].set_xlabel('Visitor Type', fontsize=10)\naxes[0, 1].set_ylabel('Proportion', fontsize=10)\naxes[0, 1].legend(['No Purchase', 'Purchase'])\naxes[0, 1].tick_params(axis='x', rotation=45)\n\n# Weekend vs Revenue\npd.crosstab(df['Weekend'], df['Revenue'], normalize='index').plot(kind='bar', ax=axes[1, 0],\n                                                                    color=['#FF6B6B', '#4ECDC4'])\naxes[1, 0].set_title('Purchase Rate by Weekend', fontsize=12, fontweight='bold')\naxes[1, 0].set_xlabel('Weekend', fontsize=10)\naxes[1, 0].set_ylabel('Proportion', fontsize=10)\naxes[1, 0].legend(['No Purchase', 'Purchase'])\naxes[1, 0].set_xticklabels(['Weekday', 'Weekend'], rotation=0)\n\n# TrafficType distribution\ntraffic_revenue = df.groupby('TrafficType')['Revenue'].value_counts(normalize=True).unstack()\ntraffic_revenue.plot(kind='bar', ax=axes[1, 1], color=['#FF6B6B', '#4ECDC4'])\naxes[1, 1].set_title('Purchase Rate by Traffic Type', fontsize=12, fontweight='bold')\naxes[1, 1].set_xlabel('Traffic Type', fontsize=10)\naxes[1, 1].set_ylabel('Proportion', fontsize=10)\naxes[1, 1].legend(['No Purchase', 'Purchase'])\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Compare key features between purchasers and non-purchasers\nkey_features = ['PageValues', 'ProductRelated_Duration', 'ExitRates', 'BounceRates']\n\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\naxes = axes.ravel()\n\nfor idx, feature in enumerate(key_features):\n    df.boxplot(column=feature, by='Revenue', ax=axes[idx])\n    axes[idx].set_title(f'{feature} by Revenue', fontsize=12, fontweight='bold')\n    axes[idx].set_xlabel('Revenue (Purchase)', fontsize=10)\n    axes[idx].set_ylabel(feature, fontsize=10)\n    \nplt.suptitle('Key Features Comparison: Purchasers vs Non-Purchasers', \n             fontsize=16, fontweight='bold', y=1.00)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Correlation analysis\n# Convert boolean Revenue to numeric for correlation\ndf_corr = df.copy()\ndf_corr['Revenue'] = df_corr['Revenue'].astype(int)\n\n# Select only numerical columns for correlation\nnumerical_features = df_corr.select_dtypes(include=[np.number]).columns.tolist()\ncorrelation_matrix = df_corr[numerical_features].corr()\n\n# Plot correlation heatmap\nplt.figure(figsize=(14, 10))\nsns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\nplt.title('Correlation Heatmap of Numerical Features', fontsize=16, fontweight='bold', pad=20)\nplt.tight_layout()\nplt.show()\n\n# Show features most correlated with Revenue\nprint(\"Features most correlated with Revenue:\")\nrevenue_corr = correlation_matrix['Revenue'].sort_values(ascending=False)\nprint(revenue_corr)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Distribution of numerical features\nnumerical_cols = ['Administrative', 'Administrative_Duration', 'Informational', \n                  'Informational_Duration', 'ProductRelated', 'ProductRelated_Duration',\n                  'BounceRates', 'ExitRates', 'PageValues', 'SpecialDay']\n\nfig, axes = plt.subplots(5, 2, figsize=(15, 20))\naxes = axes.ravel()\n\nfor idx, col in enumerate(numerical_cols):\n    axes[idx].hist(df[col], bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n    axes[idx].set_title(f'Distribution of {col}', fontsize=12, fontweight='bold')\n    axes[idx].set_xlabel(col, fontsize=10)\n    axes[idx].set_ylabel('Frequency', fontsize=10)\n    axes[idx].grid(axis='y', alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# 2. Exploratory Data Analysis (EDA)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Target variable distribution\nprint(\"Target Variable (Revenue) Distribution:\")\nprint(df['Revenue'].value_counts())\nprint(\"\\nPercentage Distribution:\")\nprint(df['Revenue'].value_counts(normalize=True) * 100)\n\n# Visualize target distribution\nfig, ax = plt.subplots(1, 2, figsize=(14, 5))\n\n# Count plot\ndf['Revenue'].value_counts().plot(kind='bar', ax=ax[0], color=['#FF6B6B', '#4ECDC4'])\nax[0].set_title('Revenue Distribution (Count)', fontsize=14, fontweight='bold')\nax[0].set_xlabel('Revenue', fontsize=12)\nax[0].set_ylabel('Count', fontsize=12)\nax[0].set_xticklabels(['No Purchase (False)', 'Purchase (True)'], rotation=0)\n\n# Pie chart\ndf['Revenue'].value_counts().plot(kind='pie', ax=ax[1], autopct='%1.1f%%', \n                                   colors=['#FF6B6B', '#4ECDC4'], startangle=90)\nax[1].set_title('Revenue Distribution (Percentage)', fontsize=14, fontweight='bold')\nax[1].set_ylabel('')\n\nplt.tight_layout()\nplt.show()\n\n# Calculate class imbalance ratio\nimbalance_ratio = df['Revenue'].value_counts()[False] / df['Revenue'].value_counts()[True]\nprint(f\"\\nClass Imbalance Ratio: {imbalance_ratio:.2f}:1 (No Purchase : Purchase)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Analyze categorical features\ncategorical_cols = ['Month', 'OperatingSystems', 'Browser', 'Region', 'TrafficType', 'VisitorType', 'Weekend', 'Revenue']\n\nprint(\"Categorical Features Analysis:\")\nfor col in categorical_cols:\n    print(f\"\\n{col}:\")\n    print(df[col].value_counts())\n    print(\"-\" * 40)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Descriptive statistics for numerical features\nprint(\"Descriptive Statistics for Numerical Features:\")\ndf.describe().T",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Display data information\nprint(\"Data Types and Non-Null Counts:\")\nprint(df.info())\nprint(\"\\n\" + \"=\"*50)\nprint(\"\\nMissing Values:\")\nprint(df.isnull().sum())\nprint(\"\\n\" + \"=\"*50)\nprint(\"\\nDuplicate Rows:\", df.duplicated().sum())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Combine features and target into a single dataframe for easier analysis\ndf = X.copy()\ndf['Revenue'] = y\n\nprint(\"Dataset Shape:\", df.shape)\nprint(\"\\nFirst few rows:\")\ndf.head()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set visualization style\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (12, 6)\n\nprint(\"Libraries imported successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# 1. Data Exploration & Understanding",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# 7. Key Insights & Conclusions\n\n## Dataset Summary\n- **Total Sessions**: 12,330\n- **Features**: 17 (10 numerical, 8 categorical)\n- **Target**: Revenue (Purchase vs No Purchase)\n- **Class Distribution**: 84.5% No Purchase, 15.5% Purchase (Imbalanced)\n\n## Key Findings from EDA\n1. **PageValues** shows strong correlation with purchase behavior\n2. **ProductRelated_Duration** - customers who purchase spend more time on product pages\n3. **ExitRates** and **BounceRates** are lower for customers who make purchases\n4. **Returning visitors** have higher purchase rates than new visitors\n5. **November and May** show higher purchase rates (likely due to shopping seasons)\n\n## Model Performance\n- Multiple classification algorithms were trained and evaluated\n- Best performing models typically include ensemble methods (Random Forest, XGBoost, Gradient Boosting)\n- Key metrics: Accuracy, Precision, Recall, F1-Score, ROC-AUC\n\n## Important Features for Prediction\nBased on feature importance analysis:\n1. **PageValues** - Most important predictor\n2. **ProductRelated_Duration** - Time spent on product pages\n3. **ExitRates** - Exit rate metrics\n4. **Month** - Seasonal effects\n5. **BounceRates** - Bounce rate metrics\n\n## Recommendations\n1. **Focus on PageValue optimization** - This is the strongest predictor\n2. **Improve product page engagement** - Longer duration correlates with purchases\n3. **Reduce exit and bounce rates** - Implement strategies to keep visitors engaged\n4. **Target returning visitors** - They have higher conversion rates\n5. **Optimize for peak seasons** - Focus marketing efforts in November and May",
   "metadata": {
    "id": "Gbftn4mkbUXu"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}