{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online Shoppers Purchasing Intention Dataset - Data Science Analysis\n",
    "\n",
    "**Dataset Source:** https://archive.ics.uci.edu/dataset/468/online+shoppers+purchasing+intention+dataset\n",
    "\n",
    "## Dataset Overview\n",
    "- **Total Sessions:** 12,330\n",
    "- **Features:** 17 (10 numerical, 8 categorical)\n",
    "- **Target:** Revenue (Purchase vs No Purchase)\n",
    "- **Class Distribution:** 84.5% No Purchase (10,422), 15.5% Purchase (1,908)\n",
    "- **Task:** Classification\n",
    "- **Missing Values:** No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required package\n",
    "!pip install ucimlrepo xgboost -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch dataset from UCI Repository\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "online_shoppers_purchasing_intention_dataset = fetch_ucirepo(id=468)\n",
    "\n",
    "# Data (as pandas dataframes)\n",
    "X = online_shoppers_purchasing_intention_dataset.data.features\n",
    "y = online_shoppers_purchasing_intention_dataset.data.targets\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, \n",
    "    roc_auc_score, roc_curve,\n",
    "    accuracy_score, precision_score, \n",
    "    recall_score, f1_score\n",
    ")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úì All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Exploration & Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine features and target into a single dataframe\n",
    "df = X.copy()\n",
    "df['Revenue'] = y\n",
    "\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nColumn Names:\")\n",
    "print(df.columns.tolist())\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display data information\n",
    "print(\"=\" * 70)\n",
    "print(\"DATA INFORMATION\")\n",
    "print(\"=\" * 70)\n",
    "df.info()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MISSING VALUES\")\n",
    "print(\"=\" * 70)\n",
    "missing = df.isnull().sum()\n",
    "print(missing[missing > 0] if missing.sum() > 0 else \"No missing values found!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DUPLICATE ROWS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Number of duplicate rows: {df.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics for numerical features\n",
    "print(\"=\" * 100)\n",
    "print(\"DESCRIPTIVE STATISTICS FOR NUMERICAL FEATURES\")\n",
    "print(\"=\" * 100)\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable distribution\n",
    "print(\"=\" * 70)\n",
    "print(\"TARGET VARIABLE (REVENUE) DISTRIBUTION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nCount:\")\n",
    "print(df['Revenue'].value_counts())\n",
    "print(\"\\nPercentage:\")\n",
    "print(df['Revenue'].value_counts(normalize=True) * 100)\n",
    "\n",
    "# Visualize target distribution\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Count plot\n",
    "df['Revenue'].value_counts().plot(kind='bar', ax=ax[0], color=['#FF6B6B', '#4ECDC4'])\n",
    "ax[0].set_title('Revenue Distribution (Count)', fontsize=14, fontweight='bold')\n",
    "ax[0].set_xlabel('Revenue', fontsize=12)\n",
    "ax[0].set_ylabel('Count', fontsize=12)\n",
    "ax[0].set_xticklabels(['No Purchase', 'Purchase'], rotation=0)\n",
    "for i, v in enumerate(df['Revenue'].value_counts()):\n",
    "    ax[0].text(i, v + 200, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "# Pie chart\n",
    "df['Revenue'].value_counts().plot(kind='pie', ax=ax[1], autopct='%1.1f%%', \n",
    "                                   colors=['#FF6B6B', '#4ECDC4'], startangle=90)\n",
    "ax[1].set_title('Revenue Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
    "ax[1].set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Class imbalance ratio\n",
    "imbalance_ratio = df['Revenue'].value_counts()[False] / df['Revenue'].value_counts()[True]\n",
    "print(f\"\\n‚ö†Ô∏è  Class Imbalance Ratio: {imbalance_ratio:.2f}:1 (No Purchase : Purchase)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of numerical features\n",
    "numerical_cols = ['Administrative', 'Administrative_Duration', 'Informational', \n",
    "                  'Informational_Duration', 'ProductRelated', 'ProductRelated_Duration',\n",
    "                  'BounceRates', 'ExitRates', 'PageValues', 'SpecialDay']\n",
    "\n",
    "fig, axes = plt.subplots(5, 2, figsize=(15, 20))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(numerical_cols):\n",
    "    axes[idx].hist(df[col], bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_title(f'Distribution of {col}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel(col, fontsize=10)\n",
    "    axes[idx].set_ylabel('Frequency', fontsize=10)\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.suptitle('Distribution of Numerical Features', fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "df_corr = df.copy()\n",
    "df_corr['Revenue'] = df_corr['Revenue'].astype(int)\n",
    "\n",
    "# Select numerical columns\n",
    "numerical_features = df_corr.select_dtypes(include=[np.number]).columns.tolist()\n",
    "correlation_matrix = df_corr[numerical_features].corr()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Correlation Heatmap of Numerical Features', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Features most correlated with Revenue\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FEATURES MOST CORRELATED WITH REVENUE\")\n",
    "print(\"=\" * 70)\n",
    "revenue_corr = correlation_matrix['Revenue'].sort_values(ascending=False)\n",
    "print(revenue_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare key features between purchasers and non-purchasers\n",
    "key_features = ['PageValues', 'ProductRelated_Duration', 'ExitRates', 'BounceRates']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(key_features):\n",
    "    df.boxplot(column=feature, by='Revenue', ax=axes[idx])\n",
    "    axes[idx].set_title(f'{feature} by Revenue', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Revenue (Purchase)', fontsize=10)\n",
    "    axes[idx].set_ylabel(feature, fontsize=10)\n",
    "    \n",
    "plt.suptitle('Key Features Comparison: Purchasers vs Non-Purchasers', \n",
    "             fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features vs Revenue\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# Month vs Revenue\n",
    "pd.crosstab(df['Month'], df['Revenue'], normalize='index').plot(\n",
    "    kind='bar', ax=axes[0, 0], color=['#FF6B6B', '#4ECDC4'])\n",
    "axes[0, 0].set_title('Purchase Rate by Month', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Month', fontsize=10)\n",
    "axes[0, 0].set_ylabel('Proportion', fontsize=10)\n",
    "axes[0, 0].legend(['No Purchase', 'Purchase'])\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# VisitorType vs Revenue\n",
    "pd.crosstab(df['VisitorType'], df['Revenue'], normalize='index').plot(\n",
    "    kind='bar', ax=axes[0, 1], color=['#FF6B6B', '#4ECDC4'])\n",
    "axes[0, 1].set_title('Purchase Rate by Visitor Type', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Visitor Type', fontsize=10)\n",
    "axes[0, 1].set_ylabel('Proportion', fontsize=10)\n",
    "axes[0, 1].legend(['No Purchase', 'Purchase'])\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Weekend vs Revenue\n",
    "pd.crosstab(df['Weekend'], df['Revenue'], normalize='index').plot(\n",
    "    kind='bar', ax=axes[1, 0], color=['#FF6B6B', '#4ECDC4'])\n",
    "axes[1, 0].set_title('Purchase Rate by Weekend', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Weekend', fontsize=10)\n",
    "axes[1, 0].set_ylabel('Proportion', fontsize=10)\n",
    "axes[1, 0].legend(['No Purchase', 'Purchase'])\n",
    "axes[1, 0].set_xticklabels(['Weekday', 'Weekend'], rotation=0)\n",
    "\n",
    "# TrafficType distribution (top 10)\n",
    "top_traffic = df['TrafficType'].value_counts().head(10).index\n",
    "df_top_traffic = df[df['TrafficType'].isin(top_traffic)]\n",
    "pd.crosstab(df_top_traffic['TrafficType'], df_top_traffic['Revenue'], normalize='index').plot(\n",
    "    kind='bar', ax=axes[1, 1], color=['#FF6B6B', '#4ECDC4'])\n",
    "axes[1, 1].set_title('Purchase Rate by Traffic Type (Top 10)', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Traffic Type', fontsize=10)\n",
    "axes[1, 1].set_ylabel('Proportion', fontsize=10)\n",
    "axes[1, 1].legend(['No Purchase', 'Purchase'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessing copy\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Separate features and target\n",
    "X_prep = df_processed.drop('Revenue', axis=1)\n",
    "y_prep = df_processed['Revenue']\n",
    "\n",
    "print(\"Original Features Shape:\", X_prep.shape)\n",
    "print(\"Target Shape:\", y_prep.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "print(\"Encoding categorical variables...\\n\")\n",
    "\n",
    "# One-hot encoding for Month\n",
    "month_encoded = pd.get_dummies(X_prep['Month'], prefix='Month', drop_first=True)\n",
    "print(f\"Month encoded: {month_encoded.shape[1]} features\")\n",
    "\n",
    "# One-hot encoding for VisitorType\n",
    "visitor_encoded = pd.get_dummies(X_prep['VisitorType'], prefix='VisitorType', drop_first=True)\n",
    "print(f\"VisitorType encoded: {visitor_encoded.shape[1]} features\")\n",
    "\n",
    "# Convert Weekend to integer\n",
    "X_prep['Weekend'] = X_prep['Weekend'].astype(int)\n",
    "\n",
    "# Drop original categorical columns\n",
    "X_prep = X_prep.drop(['Month', 'VisitorType'], axis=1)\n",
    "\n",
    "# Concatenate encoded features\n",
    "X_prep = pd.concat([X_prep, month_encoded, visitor_encoded], axis=1)\n",
    "\n",
    "# Convert target to integer\n",
    "y_prep = y_prep.astype(int)\n",
    "\n",
    "print(f\"\\n‚úì Preprocessing complete!\")\n",
    "print(f\"Final Features Shape: {X_prep.shape}\")\n",
    "print(f\"Total Features: {X_prep.shape[1]}\")\n",
    "print(f\"\\nFeature Names:\\n{X_prep.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_prep, y_prep, test_size=0.2, random_state=42, stratify=y_prep\n",
    ")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TRAIN-TEST SPLIT\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Testing set: {X_test.shape}\")\n",
    "print(f\"\\nTraining target distribution:\\n{y_train.value_counts()}\")\n",
    "print(f\"\\nTesting target distribution:\\n{y_test.value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrame\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print(\"‚úì Feature scaling complete!\")\n",
    "print(f\"Scaled training set: {X_train_scaled.shape}\")\n",
    "print(f\"Scaled testing set: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Building & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'XGBoost': XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss'),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Support Vector Machine': SVC(kernel='rbf', random_state=42, probability=True)\n",
    "}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MODELS TO TRAIN\")\n",
    "print(\"=\" * 70)\n",
    "for i, name in enumerate(models.keys(), 1):\n",
    "    print(f\"{i}. {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate all models\n",
    "results = []\n",
    "trained_models = {}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING MODELS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n[{list(models.keys()).index(name) + 1}/{len(models)}] Training {name}...\", end=\" \")\n",
    "    \n",
    "    # Train\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else None\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'ROC-AUC': roc_auc\n",
    "    })\n",
    "    \n",
    "    trained_models[name] = model\n",
    "    \n",
    "    print(f\"‚úì Done!\")\n",
    "    print(f\"    Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f} | F1: {f1:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úì ALL MODELS TRAINED SUCCESSFULLY!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Model Evaluation & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results dataframe\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('F1-Score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 100)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Best model\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "print(f\"\\nüèÜ BEST MODEL (based on F1-Score): {best_model_name}\")\n",
    "print(f\"   F1-Score: {results_df.iloc[0]['F1-Score']:.4f}\")\n",
    "print(f\"   Accuracy: {results_df.iloc[0]['Accuracy']:.4f}\")\n",
    "print(f\"   Precision: {results_df.iloc[0]['Precision']:.4f}\")\n",
    "print(f\"   Recall: {results_df.iloc[0]['Recall']:.4f}\")\n",
    "print(f\"   ROC-AUC: {results_df.iloc[0]['ROC-AUC']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    results_sorted = results_df.sort_values(metric, ascending=True)\n",
    "    \n",
    "    bars = ax.barh(results_sorted['Model'], results_sorted[metric], \n",
    "                   color=colors[idx], alpha=0.7)\n",
    "    ax.set_xlabel(metric, fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'Model Comparison - {metric}', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(results_sorted[metric]):\n",
    "        ax.text(v + 0.01, i, f'{v:.3f}', va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed evaluation of best model\n",
    "best_model = trained_models[best_model_name]\n",
    "y_pred_best = best_model.predict(X_test_scaled)\n",
    "y_pred_proba_best = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Confusion Matrix and ROC Curve\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_best)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba_best)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Confusion Matrix\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0], \n",
    "            xticklabels=['No Purchase', 'Purchase'],\n",
    "            yticklabels=['No Purchase', 'Purchase'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "axes[0].set_title(f'Confusion Matrix - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Actual', fontsize=12)\n",
    "axes[0].set_xlabel('Predicted', fontsize=12)\n",
    "\n",
    "# ROC Curve\n",
    "axes[1].plot(fpr, tpr, color='darkorange', lw=2, \n",
    "             label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "axes[1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', \n",
    "             label='Random Classifier (AUC = 0.500)')\n",
    "axes[1].set_xlim([0.0, 1.0])\n",
    "axes[1].set_ylim([0.0, 1.05])\n",
    "axes[1].set_xlabel('False Positive Rate', fontsize=12)\n",
    "axes[1].set_ylabel('True Positive Rate', fontsize=12)\n",
    "axes[1].set_title(f'ROC Curve - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(loc=\"lower right\", fontsize=10)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"CLASSIFICATION REPORT - {best_model_name}\")\n",
    "print(\"=\" * 70)\n",
    "print(classification_report(y_test, y_pred_best, \n",
    "                          target_names=['No Purchase', 'Purchase']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for tree-based models\n",
    "tree_based_models = ['Random Forest', 'Gradient Boosting', 'XGBoost', 'Decision Tree']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, model_name in enumerate(tree_based_models):\n",
    "    if model_name in trained_models:\n",
    "        model = trained_models[model_name]\n",
    "        \n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importances = model.feature_importances_\n",
    "            \n",
    "            # Create dataframe\n",
    "            feature_imp_df = pd.DataFrame({\n",
    "                'Feature': X_train.columns,\n",
    "                'Importance': importances\n",
    "            }).sort_values('Importance', ascending=False).head(15)\n",
    "            \n",
    "            # Plot\n",
    "            bars = axes[idx].barh(range(len(feature_imp_df)), \n",
    "                                 feature_imp_df['Importance'], \n",
    "                                 color='steelblue', alpha=0.7)\n",
    "            axes[idx].set_yticks(range(len(feature_imp_df)))\n",
    "            axes[idx].set_yticklabels(feature_imp_df['Feature'])\n",
    "            axes[idx].set_xlabel('Importance', fontsize=11, fontweight='bold')\n",
    "            axes[idx].set_title(f'Top 15 Features - {model_name}', \n",
    "                               fontsize=12, fontweight='bold')\n",
    "            axes[idx].invert_yaxis()\n",
    "            axes[idx].grid(axis='x', alpha=0.3)\n",
    "            \n",
    "            # Add value labels\n",
    "            for i, v in enumerate(feature_imp_df['Importance']):\n",
    "                axes[idx].text(v, i, f' {v:.3f}', va='center', fontsize=8)\n",
    "\n",
    "plt.suptitle('Feature Importance Analysis - Tree-Based Models', \n",
    "             fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Key Insights & Conclusions\n",
    "\n",
    "## Dataset Summary\n",
    "- **Total Sessions**: 12,330\n",
    "- **Features**: 17 (10 numerical, 8 categorical)\n",
    "- **Target**: Revenue (Purchase vs No Purchase)\n",
    "- **Class Distribution**: 84.5% No Purchase, 15.5% Purchase (Imbalanced)\n",
    "\n",
    "## Key Findings from EDA\n",
    "1. **PageValues** shows the strongest correlation with purchase behavior\n",
    "2. **ProductRelated_Duration** - Customers who purchase spend more time on product pages\n",
    "3. **ExitRates** and **BounceRates** are significantly lower for customers who make purchases\n",
    "4. **Returning visitors** have notably higher purchase rates compared to new visitors\n",
    "5. **November and May** show higher purchase rates (likely due to holiday shopping seasons)\n",
    "6. **Weekend** visits show slightly different conversion patterns\n",
    "\n",
    "## Model Performance Summary\n",
    "- **Best performing models**: Ensemble methods (Random Forest, XGBoost, Gradient Boosting)\n",
    "- **Key metrics**: Models evaluated on Accuracy, Precision, Recall, F1-Score, and ROC-AUC\n",
    "- **Class imbalance** was preserved through stratified splitting\n",
    "\n",
    "## Most Important Features for Prediction\n",
    "Based on feature importance analysis across tree-based models:\n",
    "1. **PageValues** - Consistently the most important predictor\n",
    "2. **ProductRelated_Duration** - Time spent browsing products\n",
    "3. **ExitRates** - Likelihood of leaving the site\n",
    "4. **BounceRates** - Single-page visit metrics\n",
    "5. **Month** - Seasonal shopping patterns\n",
    "\n",
    "## Business Recommendations\n",
    "\n",
    "### 1. Optimize Page Value Metrics\n",
    "- Focus on improving page value scores as they are the strongest predictor\n",
    "- Analyze high-value pages and replicate successful elements\n",
    "\n",
    "### 2. Enhance Product Page Engagement\n",
    "- Improve product page content and user experience\n",
    "- Add engaging elements to increase time spent on product pages\n",
    "- Implement better product recommendations\n",
    "\n",
    "### 3. Reduce Exit and Bounce Rates\n",
    "- Implement exit-intent popups with special offers\n",
    "- Improve page load times and mobile responsiveness\n",
    "- Enhance navigation and internal linking\n",
    "\n",
    "### 4. Target Returning Visitors\n",
    "- Develop loyalty programs to encourage repeat visits\n",
    "- Personalize content for returning customers\n",
    "- Implement email marketing for customer retention\n",
    "\n",
    "### 5. Seasonal Optimization\n",
    "- Increase marketing budget during peak months (November, May)\n",
    "- Prepare inventory and special promotions for high-conversion periods\n",
    "- Plan campaigns around shopping holidays\n",
    "\n",
    "### 6. Traffic Source Analysis\n",
    "- Analyze which traffic types convert best\n",
    "- Allocate marketing budget to high-converting traffic sources\n",
    "- Optimize campaigns for different traffic types\n",
    "\n",
    "## Technical Considerations\n",
    "- **Class Imbalance**: Consider using techniques like SMOTE, class weights, or ensemble methods for production\n",
    "- **Real-time Prediction**: The model can be deployed for real-time purchase intention prediction\n",
    "- **Model Updates**: Retrain periodically to adapt to changing user behavior patterns\n",
    "- **Feature Engineering**: Additional features could be created from existing ones (e.g., ratios, interactions)\n",
    "\n",
    "## Next Steps\n",
    "1. Hyperparameter tuning for the best models\n",
    "2. Handle class imbalance with advanced techniques\n",
    "3. Feature engineering for additional insights\n",
    "4. Deploy the model in a production environment\n",
    "5. Monitor model performance over time\n",
    "6. A/B testing of business recommendations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
